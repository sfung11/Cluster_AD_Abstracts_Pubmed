{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This project focuses on unsupervised learning using Pubmed abstracts on Alzheimer's Disease.  \n",
    "#### 1. Data collection step from PubMed using Biopython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True True 50580\n"
     ]
    }
   ],
   "source": [
    "# connect to aws\n",
    "from sshtunnel import SSHTunnelForwarder\n",
    "\n",
    "# SSH config shortcut\n",
    "SSH_CONFIG_SHORTCUT = 'if applicable'\n",
    "\n",
    "server = SSHTunnelForwarder(\n",
    "    SSH_CONFIG_SHORTCUT,\n",
    "    ssh_config_file='~/.ssh/config',\n",
    "    remote_bind_address=('localhost', 5432)\n",
    ")\n",
    "\n",
    "server.start()\n",
    "print(server.is_active, server.is_alive, server.local_bind_port)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "client = MongoClient(port=12345) # this is the port set by the SSH tunnel\n",
    "db = client.pubmed_ad # database name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper funcs to get data to mongoDB\n",
    "import time\n",
    "from Bio import Entrez, Medline\n",
    "import json\n",
    "alz = db.alz\n",
    "\n",
    "def filter_record(record):\n",
    "    return record\n",
    "\n",
    "def write_to_mongo(record):\n",
    "    alz.insert_one(record)\n",
    "\n",
    "def write_to_file(f, record):\n",
    "    record = {k: v for k, v in record.items() if k != '_id'}\n",
    "    f.write(json.dumps(record))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'is': 5, 'id': [1, 4, 5]}\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can you stringify a dict?\n",
    "test = {'is': 5, 'id': [1,4, 5]}\n",
    "str(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 130672 results\n",
      "Going to download record 1 to 1000\n",
      "Going to download record 1001 to 2000\n",
      "Going to download record 2001 to 3000\n",
      "Going to download record 3001 to 4000\n",
      "Going to download record 4001 to 5000\n",
      "Going to download record 5001 to 6000\n",
      "Going to download record 6001 to 7000\n",
      "Going to download record 7001 to 8000\n",
      "Going to download record 8001 to 9000\n",
      "Going to download record 9001 to 10000\n",
      "Going to download record 10001 to 11000\n",
      "Going to download record 11001 to 12000\n",
      "Going to download record 12001 to 13000\n",
      "Going to download record 13001 to 14000\n",
      "Going to download record 14001 to 15000\n",
      "Going to download record 15001 to 16000\n",
      "Going to download record 16001 to 17000\n",
      "Going to download record 17001 to 18000\n",
      "Going to download record 18001 to 19000\n",
      "Going to download record 19001 to 20000\n",
      "Going to download record 20001 to 21000\n",
      "Going to download record 21001 to 22000\n",
      "Going to download record 22001 to 23000\n",
      "Going to download record 23001 to 24000\n",
      "Going to download record 24001 to 25000\n",
      "Going to download record 25001 to 26000\n",
      "Going to download record 26001 to 27000\n",
      "Going to download record 27001 to 28000\n",
      "Going to download record 28001 to 29000\n",
      "Going to download record 29001 to 30000\n",
      "Going to download record 30001 to 31000\n",
      "Going to download record 31001 to 32000\n",
      "Going to download record 32001 to 33000\n",
      "Going to download record 33001 to 34000\n",
      "Going to download record 34001 to 35000\n",
      "Going to download record 35001 to 36000\n",
      "Going to download record 36001 to 37000\n",
      "Going to download record 37001 to 38000\n",
      "Going to download record 38001 to 39000\n",
      "Going to download record 39001 to 40000\n",
      "Going to download record 40001 to 41000\n",
      "Going to download record 41001 to 42000\n",
      "Going to download record 42001 to 43000\n",
      "Going to download record 43001 to 44000\n",
      "Going to download record 44001 to 45000\n",
      "Going to download record 45001 to 46000\n",
      "Going to download record 46001 to 47000\n",
      "Going to download record 47001 to 48000\n",
      "Going to download record 48001 to 49000\n",
      "Going to download record 49001 to 50000\n",
      "Going to download record 50001 to 51000\n",
      "Going to download record 51001 to 52000\n",
      "Going to download record 52001 to 53000\n",
      "Going to download record 53001 to 54000\n",
      "Going to download record 54001 to 55000\n",
      "Going to download record 55001 to 56000\n",
      "Going to download record 56001 to 57000\n",
      "Going to download record 57001 to 58000\n",
      "Going to download record 58001 to 59000\n",
      "Going to download record 59001 to 60000\n",
      "Going to download record 60001 to 61000\n",
      "Going to download record 61001 to 62000\n",
      "Going to download record 62001 to 63000\n",
      "Going to download record 63001 to 64000\n",
      "Going to download record 64001 to 65000\n",
      "Going to download record 65001 to 66000\n",
      "Going to download record 66001 to 67000\n",
      "Going to download record 67001 to 68000\n",
      "Going to download record 68001 to 69000\n",
      "Going to download record 69001 to 70000\n",
      "Going to download record 70001 to 71000\n",
      "Going to download record 71001 to 72000\n",
      "Going to download record 72001 to 73000\n",
      "Going to download record 73001 to 74000\n",
      "Going to download record 74001 to 75000\n",
      "Going to download record 75001 to 76000\n",
      "Going to download record 76001 to 77000\n",
      "Going to download record 77001 to 78000\n",
      "Going to download record 78001 to 79000\n",
      "Going to download record 79001 to 80000\n",
      "Going to download record 80001 to 81000\n",
      "Going to download record 81001 to 82000\n",
      "Going to download record 82001 to 83000\n",
      "Going to download record 83001 to 84000\n",
      "Going to download record 84001 to 85000\n",
      "Going to download record 85001 to 86000\n",
      "Going to download record 86001 to 87000\n",
      "Going to download record 87001 to 88000\n",
      "Going to download record 88001 to 89000\n",
      "Going to download record 89001 to 90000\n",
      "Going to download record 90001 to 91000\n",
      "Going to download record 91001 to 92000\n",
      "Going to download record 92001 to 93000\n",
      "Going to download record 93001 to 94000\n",
      "Going to download record 94001 to 95000\n",
      "Going to download record 95001 to 96000\n",
      "Going to download record 96001 to 97000\n",
      "Going to download record 97001 to 98000\n",
      "Going to download record 98001 to 99000\n",
      "Going to download record 99001 to 100000\n",
      "Going to download record 100001 to 101000\n",
      "Going to download record 101001 to 102000\n",
      "Going to download record 102001 to 103000\n",
      "Going to download record 103001 to 104000\n",
      "Going to download record 104001 to 105000\n",
      "Going to download record 105001 to 106000\n",
      "Going to download record 106001 to 107000\n",
      "Going to download record 107001 to 108000\n",
      "Going to download record 108001 to 109000\n",
      "Going to download record 109001 to 110000\n",
      "Going to download record 110001 to 111000\n",
      "Going to download record 111001 to 112000\n",
      "Going to download record 112001 to 113000\n",
      "Going to download record 113001 to 114000\n",
      "Going to download record 114001 to 115000\n",
      "Going to download record 115001 to 116000\n",
      "Going to download record 116001 to 117000\n",
      "Going to download record 117001 to 118000\n",
      "Going to download record 118001 to 119000\n",
      "Going to download record 119001 to 120000\n",
      "Going to download record 120001 to 121000\n",
      "Going to download record 121001 to 122000\n",
      "Going to download record 122001 to 123000\n",
      "Going to download record 123001 to 124000\n",
      "Going to download record 124001 to 125000\n",
      "Going to download record 125001 to 126000\n",
      "Going to download record 126001 to 127000\n",
      "Going to download record 127001 to 128000\n",
      "Going to download record 128001 to 129000\n",
      "Going to download record 129001 to 130000\n",
      "Going to download record 130001 to 130672\n"
     ]
    }
   ],
   "source": [
    "# modified from biopython docs. Errors in documentation! \n",
    "\n",
    "from urllib.error import HTTPError  # for Python 3\n",
    "\n",
    "Entrez.email = \"your_email@gmail.com\"\n",
    "search_results = Entrez.read(Entrez.esearch(db=\"pubmed\",\n",
    "                                            term=\"alzheimer\\'s\",\n",
    "                                            usehistory=\"y\"))\n",
    "count = int(search_results[\"Count\"])\n",
    "print(\"Found %i results\" % count)\n",
    "batch_size = 1000\n",
    "\n",
    "# two lines below to test\n",
    "# count = 6\n",
    "# batch_size = 2\n",
    "\n",
    "\n",
    "out_handle = open('backup.json', 'a')\n",
    "for start in range(0,count,batch_size):\n",
    "    time.sleep(0.3)\n",
    "    end = min(count, start+batch_size)\n",
    "    print(\"Going to download record %i to %i\" % (start+1, end))\n",
    "    attempt = 1\n",
    "    while attempt <= 3:\n",
    "        try:\n",
    "            fetch_handle = Entrez.efetch(db=\"pubmed\",rettype=\"medline\",\n",
    "                                         retmode=\"text\",retstart=start,\n",
    "                                         retmax=batch_size,\n",
    "                                         webenv=search_results[\"WebEnv\"],\n",
    "                                         query_key=search_results[\"QueryKey\"])\n",
    "            #print('finished fetch')\n",
    "            attempt = 4 # added this line else enters infinite loop\n",
    "        except HTTPError as err:\n",
    "            if 500 <= err.code <= 599:\n",
    "                print(\"Received error from server %s\" % err)\n",
    "                print(\"Attempt %i of 3\" % attempt)\n",
    "                attempt += 1\n",
    "                time.sleep(15)\n",
    "            else:\n",
    "                raise\n",
    "    records = Medline.parse(fetch_handle) # this and next 4 lines deviate from docs\n",
    "    for i, record in enumerate(records):\n",
    "        record = filter_record(record)\n",
    "        write_to_mongo(record)\n",
    "        write_to_file(out_handle, record)\n",
    "out_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wc -l backup.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['admin', 'dog', 'local', 'new_cool_db', 'pubmed_ad']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130672"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.pubmed_ad.alz.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# used this line when I was testing with smaller batches. \n",
    "#client.pubmed_ad.alz.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('598d77ccb9078c455b73ca07'), 'PMID': '28797122', 'OWN': 'NLM', 'STAT': 'Publisher', 'DA': '20170810', 'LR': '20170810', 'IS': '1553-7374 (Electronic) 1553-7366 (Linking)', 'VI': '13', 'IP': '8', 'DP': '2017 Aug 10', 'TI': 'Cross-seeding of prions by aggregated alpha-synuclein leads to transmissible spongiform encephalopathy.', 'PG': 'e1006563', 'LID': '10.1371/journal.ppat.1006563 [doi]', 'AB': \"Aggregation of misfolded proteins or peptides is a common feature of neurodegenerative diseases including Alzheimer's, Parkinson's, Huntington's, prion and other diseases. Recent years have witnessed a growing number of reports of overlap in neuropathological features that were once thought to be unique to only one neurodegenerative disorder. However, the origin for the overlap remains unclear. One possibility is that diseases with mixed brain pathologies might arise from cross-seeding of one amyloidogenic protein by aggregated states of unrelated proteins. In the current study we examined whether prion replication can be induced by cross-seeding by alpha-synuclein or Abeta peptide. We found that alpha-synuclein aggregates formed in cultured cells or in vitro display cross-seeding activity and trigger misfolding of the prion protein (PrPC) in serial Protein Misfolding Cyclic Amplification reactions, producing self-replicating PrP states characterized by a short C-terminal proteinase K (PK)-resistant region referred to as PrPres. Non-fibrillar alpha-synuclein or fibrillar Abeta failed to cross-seed misfolding of PrPC. Remarkably, PrPres triggered by aggregated alpha-synuclein in vitro propagated in animals and, upon serial transmission, produced PrPSc and clinical prion disease characterized by spongiosis and astrocytic gliosis. The current study demonstrates that aggregated alpha-synuclein is potent in cross-seeding of prion protein misfolding and aggregation in vitro, producing self-replicating states that can lead to transmissible prion diseases upon serial passaging in wild type animals. In summary, the current work documents direct cross-seeding between unrelated amyloidogenic proteins associated with different neurodegenerative diseases. This study suggests that early interaction between unrelated amyloidogenic proteins might underlie the etiology of mixed neurodegenerative proteinopathies.\", 'FAU': ['Katorcha, Elizaveta', 'Makarava, Natallia', 'Lee, Young Jin', 'Lindberg, Iris', 'Monteiro, Mervyn J', 'Kovacs, Gabor G', 'Baskakov, Ilia V'], 'AU': ['Katorcha E', 'Makarava N', 'Lee YJ', 'Lindberg I', 'Monteiro MJ', 'Kovacs GG', 'Baskakov IV'], 'AD': 'Center for Biomedical Engineering and Technology, University of Maryland School of Medicine, Baltimore, Maryland, United States of America. Center for Biomedical Engineering and Technology, University of Maryland School of Medicine, Baltimore, Maryland, United States of America. Center for Biomedical Engineering and Technology, University of Maryland School of Medicine, Baltimore, Maryland, United States of America. Department of Anatomy and Neurobiology, University of Maryland School of Medicine, Baltimore, Maryland, United States of America. Center for Biomedical Engineering and Technology, University of Maryland School of Medicine, Baltimore, Maryland, United States of America. Department of Anatomy and Neurobiology, University of Maryland School of Medicine, Baltimore, Maryland, United States of America. Institute of Neurology, Medical University of Vienna, Vienna, Austria. Center for Biomedical Engineering and Technology, University of Maryland School of Medicine, Baltimore, Maryland, United States of America. Department of Anatomy and Neurobiology, University of Maryland School of Medicine, Baltimore, Maryland, United States of America.', 'AUID': ['ORCID: http://orcid.org/0000-0002-1188-170X', 'ORCID: http://orcid.org/0000-0003-2821-0942'], 'LA': ['eng'], 'PT': ['Journal Article'], 'DEP': '20170810', 'PL': 'United States', 'TA': 'PLoS Pathog', 'JT': 'PLoS pathogens', 'JID': '101238921', 'EDAT': '2017/08/11 06:00', 'MHDA': '2017/08/11 06:00', 'CRDT': ['2017/08/11 06:00'], 'PHST': ['2017/04/24 [received]', '2017/07/31 [accepted]'], 'AID': ['10.1371/journal.ppat.1006563 [doi]', 'PPATHOGENS-D-17-00868 [pii]'], 'PST': 'aheadofprint', 'SO': 'PLoS Pathog. 2017 Aug 10;13(8):e1006563. doi: 10.1371/journal.ppat.1006563.'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in client.pubmed_ad.alz.find():\n",
    "    print(i)\n",
    "    print()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'handle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-4af01c39891b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# exit out of Bio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# close out of aws ssh tunnel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'handle' is not defined"
     ]
    }
   ],
   "source": [
    "# exit out of Bio\n",
    "handle.close()\n",
    "# close out of aws ssh tunnel\n",
    "server.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
